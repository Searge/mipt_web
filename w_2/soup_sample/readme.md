# Практическое задание по Beautiful Soup

В этом задании нужно написать функцию, которая соберет статистику по нескольким страницам из wikipedia. Сложность еще и в том, что эти страницы нужно сначала узнать.

В примере к этому заданию находится папка wiki, в ней лежит около 500 страниц из английской Википедии. Например, страница https://en.wikipedia.org/wiki/Stone_Age лежит в wiki/Stone_Age.

В вашу функцию `parse` будет передан путь (`path`) до папки wiki, и имена двух статей (start, end), например: Stone_Age и `Python_(programming_language)`. Гарантируется, что файлы обеих статей есть в папке wiki и из первой статьи можно попасть в последнюю, переходя по ссылкам только на те статьи, копии которых есть в папке wiki. В нашем примере, в статье `Stone_Age` есть ссылка на `Brain`, в ней на `Artificial_intelligence`, а в ней на `Python_(programming_language)` и это кратчайший путь от `Stone_Age` до `Python_(programming_language)`. Ваша задача — найти самый короткий путь (гарантируется, что существует только один путь минимальной длины), а затем пройтись Beautiful Soup по всем статьям в пути, для каждой найти тело статьи, это `<div id="bodyContent">`, и внутри него подсчитать:

1. Количество картинок (`img`) с шириной (`width`) не меньше `200`. Например: `<img width="200">`, но не `<img>` и не `<img width="199">`
2. Количество заголовков (`h1, h2, h3, h4, h5, h6`), первая буква текста внутри которых соответствует заглавной букве **E**, **T** или **C**. Например: `<h1>`**End**`</h1>` или `<h5><span>`**Contents**`</span></h5>`, но не `<h1>`_About_`</h1>` и не `<h2>`_end_`</h2>` и не `<h3><span>`1`</span><span>`End`</span></h3>`
3. Длину максимальной последовательности ссылок, между которыми нет других тегов, открывающихся или закрывающихся. Например: `<p><span><a></a></span>`, `<a></a>`, `<a></a></p>` - тут 2 ссылки подряд, т.к. закрывающийся `span` прерывает последовательность. `<p><a><span></span></a>, <a></a>, <a></a></p>` - а тут 3 ссылки подяд, т.к. `span` находится внутри ссылки, а не между ссылками.
4. Количество списков (`ul, ol`), не вложенных в другие списки. Например: `<ol><li></li></ol>, <ul><li><ol><li></li></ol></li></ul>` - два не вложенных списка (и один вложенный)

Результат нужно вернуть в виде словаря, ключами которого являются имена статей, а значениями списки из четырех чисел, посчитанных по формулам выше (порядок статей значения не имеет). Для нашего примера правильный результат будет :

```python
return {
    'Stone_Age': [13, 10, 12, 40],
    'Brain': [19, 5, 25, 11],
    'Artificial_intelligence': [8, 19, 13, 198],
    'Python_(programming_language)': [2, 5, 17, 41]
}
```

В пункте про последовательность ссылок вы можете ошибиться с результатом, если решите использовать метод `find_next()`. Обратите внимание, что хотя `find_next` находит тег, идущий сразу за текущим, этот тег может оказаться вложенным в текущий, а не быть его следующим соседом. Возможно, нужно использовать другой метод или алгоритм.

Так же, не упустите момент, что данные во всех пунктах нужно искать внутри `<div id="bodyContent">`, а не по всей странице.

Во время проверки на сервере будут доступны только стандартные модули и bs4, сеть не доступна, а набор статей в папке wiki, имена начальной и конечной статей будут не такие, как в примере к этому заданию. Решением будет файл wikistat.py, который нужно доработать и загрузить на сервер для проверки.

**"Важное замечание! Не используйте для сбора статистики по странице регулярные выражения. `Beautiful Soup` в своей работе использует специализированные парсеры, которые позволяют корректно обрабатывать невалидные (содержащие ошибки, например: незакрытый тег) html страницы. Статистика, собранная с помощью модуля re, на таких страницах будет возвращать не верное значение."**